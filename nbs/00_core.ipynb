{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# default_exp core"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Core\n",
    "\n",
    "> This module contains all of the core information retrieval ranking metrics from: https://gist.github.com/bwhite/3726239."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# export\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def hit_rate_at_k(rs, k):\n",
    "    \"\"\"Score is percentage of first relevant item in list that occur\n",
    "    at rank k or lower. First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    \n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: the largest rank position to consider\n",
    "    Returns:\n",
    "        Hit Rate @k\n",
    "    \"\"\"\n",
    "    if k < 1 or k > len(rs[0]):\n",
    "        raise ValueError('k value must be >=1 and < Max Rank')\n",
    "    hits = 0\n",
    "    for r in rs:\n",
    "        if np.sum(r[:k]) > 0: hits += 1\n",
    "\n",
    "    return hits / len(rs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HIT_1_VAL = 0.6666666666666666\n",
    "\n",
    "relevancies = [[0, 1], [1, 1], [1, 0]]\n",
    "h1 = hit_rate_at_k(relevancies, 1)\n",
    "\n",
    "assert HIT_1_VAL == h1\n",
    "\n",
    "HIT_2_VAL = 1.0\n",
    "\n",
    "relevancies = [[0, 1], [1, 1], [1, 0]]\n",
    "h2 = hit_rate_at_k(relevancies, 2)\n",
    "\n",
    "assert HIT_2_VAL == h2\n",
    "\n",
    "try:\n",
    "    hit_rate_at_k(relevancies, -1)\n",
    "    assert False\n",
    "except Exception as e:\n",
    "    assert type(e) == ValueError\n",
    "\n",
    "try:\n",
    "    hit_rate_at_k(relevancies, 4)\n",
    "    assert False\n",
    "except Exception as e:\n",
    "    assert type(e) == ValueError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def mean_rank(rs):\n",
    "    \"\"\"Score is mean rank of the first relevant item in list\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    \n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean rank\n",
    "    \"\"\"\n",
    "    _rs = []\n",
    "    for r in rs:\n",
    "        ids = np.asarray(r).nonzero()[0]\n",
    "        if len(ids) == 0:\n",
    "            _rs.append(0)\n",
    "        else:\n",
    "            _rs.append(ids[0] + 1)\n",
    "    return np.mean(_rs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MR_VAL = 1.3333333333333333\n",
    "\n",
    "relevancies = [[0, 1], [1, 1], [1, 0]]\n",
    "mr = mean_rank(relevancies)\n",
    "\n",
    "assert MR_VAL == mr\n",
    "\n",
    "MR_VAL = 0.3333333333333333\n",
    "\n",
    "relevancies = [[0, 1], [1, 1], [1, 0], [0, 0]]\n",
    "mr = mean_rank(relevancies)\n",
    "\n",
    "assert MR_VAL, mr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    \n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MRR_VAL = 0.61111111111111105\n",
    "relevancies = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "mrr = mean_reciprocal_rank(relevancies)\n",
    "\n",
    "assert MRR_VAL == mrr\n",
    "\n",
    "MRR_VAL = 0.5\n",
    "relevancies = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "mrr = mean_reciprocal_rank(relevancies)\n",
    "\n",
    "assert MRR_VAL == mrr\n",
    "\n",
    "MRR_VAL = 0.75\n",
    "relevancies = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "mrr = mean_reciprocal_rank(relevancies)\n",
    "\n",
    "assert MRR_VAL == mrr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def r_precision(r):\n",
    "    \"\"\"Score is precision after all relevant documents have been retrieved\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        R Precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    return np.mean(r[:z[-1] + 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PRECISION_VAL = 0.33333333333333331\n",
    "relevancy = [0, 0, 1]\n",
    "precision = r_precision(relevancy)\n",
    "\n",
    "assert PRECISION_VAL == precision\n",
    "\n",
    "PRECISION_VAL = 0.5\n",
    "relevancy = [0, 1, 0]\n",
    "precision = r_precision(relevancy)\n",
    "\n",
    "assert PRECISION_VAL == precision\n",
    "\n",
    "PRECISION_VAL = 1.0\n",
    "relevancy = [1, 0, 0]\n",
    "precision = r_precision(relevancy)\n",
    "\n",
    "assert PRECISION_VAL == precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# export\n",
    "def r_recall(r, max_rel):\n",
    "    \"\"\"Score is recall after all relevant documents have been retrieved\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        max_rel: Maximum number of documents that can be relevant\n",
    "    Returns:\n",
    "        R Recall\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    if np.sum(r) > max_rel:\n",
    "        raise ValueError('Number of relevant documents retrieved > max_rel')\n",
    "    return np.sum(r) / max_rel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "RECALL_VAL = 0.33333333333333331\n",
    "relevancy = [0, 0, 1]\n",
    "recall = r_recall(relevancy, 3)\n",
    "\n",
    "assert RECALL_VAL == recall\n",
    "\n",
    "RECALL_VAL = 0.5\n",
    "relevancy = [0, 1, 0]\n",
    "recall = r_recall(relevancy, 2)\n",
    "\n",
    "assert RECALL_VAL == recall\n",
    "\n",
    "RECALL_VAL = 0.\n",
    "relevancy = [0, 0, 0]\n",
    "recall = r_recall(relevancy, 3)\n",
    "\n",
    "assert RECALL_VAL == recall\n",
    "\n",
    "RECALL_VAL = 1.0\n",
    "relevancy = [1, 0, 0]\n",
    "recall = r_recall(relevancy, 1)\n",
    "\n",
    "assert RECALL_VAL == recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "relevancy = [1, 1, 1]\n",
    "try:\n",
    "    r_recall(relevancy, 1)\n",
    "    assert False\n",
    "except Exception as e:\n",
    "    assert type(e) == ValueError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PRECISION_K_VAL = 0.0\n",
    "relevancy = [0, 0, 1]\n",
    "precision_k = precision_at_k(relevancy, 1)\n",
    "\n",
    "assert PRECISION_K_VAL == precision_k\n",
    "\n",
    "PRECISION_K_VAL = 0.0\n",
    "precision_k = precision_at_k(relevancy, 2)\n",
    "\n",
    "assert PRECISION_K_VAL == precision_k\n",
    "\n",
    "PRECISION_K_VAL = 0.33333333333333331\n",
    "precision_k = precision_at_k(relevancy, 3)\n",
    "\n",
    "assert PRECISION_K_VAL == precision_k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    precision_at_k(relevancy, 4)\n",
    "    assert False\n",
    "except Exception as e:\n",
    "    assert type(e) == ValueError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "relevancy = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "delta_r = 1. / sum(relevancy)\n",
    "AVG_PRECISION_VAL = sum([sum(relevancy[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(relevancy) if y])\n",
    "\n",
    "avg_precision = average_precision(relevancy)\n",
    "\n",
    "assert AVG_PRECISION_VAL == avg_precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MEAN_AVG_PRECISION_VAL = 0.78333333333333333\n",
    "\n",
    "relevancies = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "mean_avg_precision = mean_average_precision(relevancies)\n",
    "\n",
    "assert MEAN_AVG_PRECISION_VAL == mean_avg_precision\n",
    "\n",
    "MEAN_AVG_PRECISION_VAL = 0.39166666666666666\n",
    "\n",
    "relevancies = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "mean_avg_precision = mean_average_precision(relevancies)\n",
    "\n",
    "assert MEAN_AVG_PRECISION_VAL == mean_avg_precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DCG_K_VAL = 3.0\n",
    "\n",
    "relevancy = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "dcg_k = dcg_at_k(relevancy, 1)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k\n",
    "\n",
    "DCG_K_VAL = 3.0\n",
    "\n",
    "dcg_k = dcg_at_k(relevancy, 1, method=1)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k\n",
    "\n",
    "DCG_K_VAL = 5.0\n",
    "\n",
    "dcg_k = dcg_at_k(relevancy, 2)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k\n",
    "\n",
    "DCG_K_VAL = 4.2618595071429155\n",
    "\n",
    "dcg_k = dcg_at_k(relevancy, 2, method=1)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k\n",
    "\n",
    "DCG_K_VAL = 9.6051177391888114\n",
    "\n",
    "dcg_k = dcg_at_k(relevancy, 10)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k\n",
    "\n",
    "DCG_K_VAL = 9.6051177391888114\n",
    "\n",
    "dcg_k = dcg_at_k(relevancy, 11)\n",
    "\n",
    "assert DCG_K_VAL == dcg_k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NDCG_K_VAL = 1.0\n",
    "\n",
    "relevance = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "ndcg_k = ndcg_at_k(relevance, 1)\n",
    "\n",
    "assert NDCG_K_VAL == ndcg_k\n",
    "\n",
    "NDCG_K_VAL = 0.9203032077642922\n",
    "\n",
    "relevance = [2, 1, 2, 0]\n",
    "ndcg_k = ndcg_at_k(relevance, 4)\n",
    "\n",
    "assert NDCG_K_VAL == ndcg_k\n",
    "\n",
    "NDCG_K_VAL = 0.96519546960144276\n",
    "\n",
    "ndcg_k = ndcg_at_k(relevance, 4, method=1)\n",
    "\n",
    "assert NDCG_K_VAL == ndcg_k\n",
    "\n",
    "NDCG_K_VAL = 0.0\n",
    "\n",
    "ndcg_k = ndcg_at_k([0], 1)\n",
    "\n",
    "assert NDCG_K_VAL == ndcg_k\n",
    "\n",
    "NDCG_K_VAL = 1.0\n",
    "\n",
    "ndcg_k = ndcg_at_k([1], 2)\n",
    "\n",
    "assert NDCG_K_VAL == ndcg_k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
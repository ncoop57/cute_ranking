{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#hide\n",
    "from cute_ranking.core import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cute Ranking\n",
    "\n",
    "> A cute little python module for calculating different ranking metrics. Based entirely on the gist from https://gist.github.com/bwhite/3726239."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/cute-ranking)](https://pypi.org/project/cute-ranking/)\n",
    "[![PyPI Status](https://badge.fury.io/py/cute-ranking.svg)](https://badge.fury.io/py/cute-ranking)\n",
    "[![PyPI Status](https://pepy.tech/badge/cute-ranking)](https://pepy.tech/project/cute-ranking)\n",
    "[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ncoop57/cute-ranking/blob/main/LICENSE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Requires a minimum python installation of 3.6\n",
    "\n",
    "`pip install cute_ranking`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from cute_ranking.core import mean_reciprocal_rank\n",
    "\n",
    "relevancies = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "mean_reciprocal_rank(relevancies)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The library current supports the following information retrieval ranking metrics:\n",
    "1. Mean Reciprocal Rank - `mean_reciprocal_rank`\n",
    "2. Relevancy Precision - `r_precision`\n",
    "3. Precision at K - `precision_at_k`\n",
    "4. Recall at K - `recall_at_k`\n",
    "5. F1 score at K - `f1_score_at_k`\n",
    "6. Average Precision - `average_precision`\n",
    "7. Mean Average Precision - `mean_average_precision`\n",
    "8. Discounted Cumulative Gain at K - `dcg_at_k`\n",
    "9. Normalized Discounted Cumulative Gain at K - `ndcg_at_k`\n",
    "10. Mean Rank - `mean_rank`\n",
    "11. Hit@k - `hit_rate_at_k`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Contributing\n",
    "PRs and issues welcome! Please make sure to read through the `CONTRIBUTING.md` doc for how to contribute :)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "version": "3.8.10",
   "name": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}